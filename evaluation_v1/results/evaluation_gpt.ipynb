{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6967954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/rishabh.singh/Desktop/ai-search-retrieval-pipeline-poc-2/app/evaluation_v1/results/mismatched_evaluation_results_gpt1.json\", 'r') as f :\n",
    "    data1 = json.load(f) \n",
    "\n",
    "with open(\"/Users/rishabh.singh/Desktop/ai-search-retrieval-pipeline-poc-2/app/evaluation_v1/results/mismatched_evaluation_results_gpt2.json\", 'r') as f :\n",
    "    data2 = json.load(f)\n",
    "\n",
    "with open(\"/Users/rishabh.singh/Desktop/ai-search-retrieval-pipeline-poc-2/app/evaluation_v1/results/mismatched_evaluation_results_gpt_base.json\", 'r') as f :\n",
    "    data3 = json.load(f)\n",
    "\n",
    "with open(\"/Users/rishabh.singh/Desktop/ai-search-retrieval-pipeline-poc-2/app/evaluation_v1/results/mismatched_evaluation_results_gpt_base_2.json\", 'r') as f :\n",
    "    data4 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd081cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== üîç GPT vs Qwen-FT Comparison Across Two Evaluation Calls ===\n",
      "\n",
      "Note: Model labels are aligned. GPT = Model 1 in both, Qwen = Model 2\n",
      "\n",
      "Comparison format: [Data1 ‚Üí Data2]\n",
      "       Comparison  Query Count\n",
      "Model 1 ‚Üí Model 1         1335\n",
      "Model 1 ‚Üí Model 2          163\n",
      "    Model 1 ‚Üí Tie          170\n",
      "Model 2 ‚Üí Model 1          191\n",
      "Model 2 ‚Üí Model 2         1091\n",
      "    Model 2 ‚Üí Tie          304\n",
      "    Tie ‚Üí Model 1          325\n",
      "    Tie ‚Üí Model 2          131\n",
      "        Tie ‚Üí Tie          778\n",
      "\n",
      "=== üìä Confusion Matrix ===\n",
      "Data2    Model 1  Model 2  Tie\n",
      "Data1                         \n",
      "Model 1     1335      163  170\n",
      "Model 2      191     1091  304\n",
      "Tie          325      131  778\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# === Invert better_model label for data2 ===\n",
    "def invert_label(label):\n",
    "    label = label.strip()\n",
    "    if label.lower() == \"model 1\":\n",
    "        return \"Model 2\"\n",
    "    elif label.lower() == \"model 2\":\n",
    "        return \"Model 1\"\n",
    "    else:\n",
    "        return \"Tie\"\n",
    "\n",
    "# === Build mapping: query -> better_model ===\n",
    "def get_better_model_map(data, invert=False):\n",
    "    mapping = {}\n",
    "    for item in data:\n",
    "        query = item[\"query\"]\n",
    "        better_model = item.get(\"better_model\", \"Tie\").strip()\n",
    "        if invert:\n",
    "            better_model = invert_label(better_model)\n",
    "        mapping[query] = better_model\n",
    "    return mapping\n",
    "\n",
    "# In data1: Model 1 = GPT, Model 2 = Qwen\n",
    "# In data2: Model 1 = Qwen, Model 2 = GPT ‚Üí so we invert data2\n",
    "bm1_map = get_better_model_map(data1, invert=False)\n",
    "bm2_map = get_better_model_map(data2, invert=True)\n",
    "\n",
    "# === Compare results for common queries ===\n",
    "counter = Counter()\n",
    "records = []\n",
    "\n",
    "common_queries = set(bm1_map.keys()) & set(bm2_map.keys())\n",
    "\n",
    "for query in common_queries:\n",
    "    bm1 = bm1_map[query]  # From data1\n",
    "    bm2 = bm2_map[query]  # From data2 (already inverted)\n",
    "    \n",
    "    counter[(bm1, bm2)] += 1\n",
    "    records.append({\n",
    "        \"query\": query,\n",
    "        \"Better Model (Data1)\": bm1,\n",
    "        \"Better Model (Data2 - inverted)\": bm2\n",
    "    })\n",
    "\n",
    "# === Display summary cleanly ===\n",
    "print(\"\\n=== üîç GPT vs Qwen-FT Comparison Across Two Evaluation Calls ===\\n\")\n",
    "print(\"Note: Model labels are aligned. GPT = Model 1 in both, Qwen = Model 2\\n\")\n",
    "print(\"Comparison format: [Data1 ‚Üí Data2]\")\n",
    "\n",
    "summary_table = []\n",
    "\n",
    "for (m1, m2), count in sorted(counter.items()):\n",
    "    label = f\"{m1} ‚Üí {m2}\"\n",
    "    summary_table.append((label, count))\n",
    "\n",
    "# Create clean DataFrame for display\n",
    "df_summary = pd.DataFrame(summary_table, columns=[\"Comparison\", \"Query Count\"])\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "df_compare = pd.DataFrame(records)\n",
    "conf_matrix = pd.crosstab(\n",
    "    df_compare[\"Better Model (Data1)\"],\n",
    "    df_compare[\"Better Model (Data2 - inverted)\"],\n",
    "    rownames=[\"Data1 \"],\n",
    "    colnames=[\"Data2 \"],\n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== üìä Confusion Matrix ===\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eca5250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== üîç GPT vs Qwen-Base Comparison Across Two Evaluation Calls ===\n",
      "\n",
      "Note: Model labels are aligned. GPT = Model 1 in both, Qwen-Base = Model 2\n",
      "\n",
      "Comparison format: [Data3 ‚Üí Data4]\n",
      "       Comparison  Query Count\n",
      "Model 1 ‚Üí Model 1         3970\n",
      "Model 1 ‚Üí Model 2          352\n",
      "    Model 1 ‚Üí Tie          375\n",
      "Model 2 ‚Üí Model 1          480\n",
      "Model 2 ‚Üí Model 2         2821\n",
      "    Model 2 ‚Üí Tie          290\n",
      "    Tie ‚Üí Model 1          704\n",
      "    Tie ‚Üí Model 2          675\n",
      "        Tie ‚Üí Tie         1254\n",
      "\n",
      "=== üìä Confusion Matrix ===\n",
      "Data4    Model 1  Model 2   Tie\n",
      "Data3                          \n",
      "Model 1     3970      352   375\n",
      "Model 2      480     2821   290\n",
      "Tie          704      675  1254\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# === Invert better_model label for data3 ===\n",
    "def invert_label(label):\n",
    "    label = label.strip()\n",
    "    if label.lower() == \"model 1\":\n",
    "        return \"Model 2\"\n",
    "    elif label.lower() == \"model 2\":\n",
    "        return \"Model 1\"\n",
    "    else:\n",
    "        return \"Tie\"\n",
    "\n",
    "# === Build mapping: query -> better_model ===\n",
    "def get_better_model_map(data, invert=False):\n",
    "    mapping = {}\n",
    "    for item in data:\n",
    "        query = item[\"query\"]\n",
    "        better_model = item.get(\"better_model\", \"Tie\").strip()\n",
    "        if invert:\n",
    "            better_model = invert_label(better_model)\n",
    "        mapping[query] = better_model\n",
    "    return mapping\n",
    "\n",
    "# In data3: Model 1 = Qwen-Base, Model 2 = GPT ‚Üí so we invert data3\n",
    "# In data4: Model 1 = GPT, Model 2 = Qwen-Base ‚Üí already aligned\n",
    "bm3_map = get_better_model_map(data3, invert=True)   # invert to make GPT = Model 1\n",
    "bm4_map = get_better_model_map(data4, invert=False)  # already aligned\n",
    "\n",
    "# === Compare results for common queries ===\n",
    "counter = Counter()\n",
    "records = []\n",
    "\n",
    "common_queries = set(bm3_map.keys()) & set(bm4_map.keys())\n",
    "\n",
    "for query in common_queries:\n",
    "    bm1 = bm3_map[query]  # From data3 (after inversion)\n",
    "    bm2 = bm4_map[query]  # From data4 (no inversion needed)\n",
    "    \n",
    "    counter[(bm1, bm2)] += 1\n",
    "    records.append({\n",
    "        \"query\": query,\n",
    "        \"Better Model (Data3)\": bm1,\n",
    "        \"Better Model (Data4)\": bm2\n",
    "    })\n",
    "\n",
    "# === Display summary cleanly ===\n",
    "print(\"\\n=== üîç GPT vs Qwen-Base Comparison Across Two Evaluation Calls ===\\n\")\n",
    "print(\"Note: Model labels are aligned. GPT = Model 1 in both, Qwen-Base = Model 2\\n\")\n",
    "print(\"Comparison format: [Data3 ‚Üí Data4]\")\n",
    "\n",
    "summary_table = []\n",
    "for (m1, m2), count in sorted(counter.items()):\n",
    "    label = f\"{m1} ‚Üí {m2}\"\n",
    "    summary_table.append((label, count))\n",
    "\n",
    "# Display as DataFrame\n",
    "df_summary = pd.DataFrame(summary_table, columns=[\"Comparison\", \"Query Count\"])\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "df_compare = pd.DataFrame(records)\n",
    "conf_matrix = pd.crosstab(\n",
    "    df_compare[\"Better Model (Data3)\"],\n",
    "    df_compare[\"Better Model (Data4)\"],\n",
    "    rownames=[\"Data3\"],\n",
    "    colnames=[\"Data4\"],\n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== üìä Confusion Matrix ===\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e25291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Format model 1 and model 2 output to be same\n",
    "\n",
    "import json\n",
    "\n",
    "# === Load input JSON ===\n",
    "input_path = \"/Users/rishabh.singh/Desktop/ai-search-retrieval-pipeline-poc-2/app/evaluation_v1/mismatched_outputs_llama.json\"   # Replace with your actual input file\n",
    "output_path = \"converted_output_llama.json\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# === Function to transform model_2 ===\n",
    "def restructure_model_2(model_2_dict):\n",
    "    if not isinstance(model_2_dict, dict):\n",
    "        print(f\"‚ö†Ô∏è Skipping model_2 ‚Äî expected dict but got {type(model_2_dict)}\")\n",
    "        return model_2_dict  # Return as-is if it's not a dict\n",
    "\n",
    "    new_model_2 = {}\n",
    "\n",
    "    for key, values in model_2_dict.items():\n",
    "        if not isinstance(values, list):\n",
    "            continue  # Skip malformed entries\n",
    "        for val in values:\n",
    "            if not isinstance(val, str):\n",
    "                continue  # Skip non-string values\n",
    "            if ':' in val:\n",
    "                parts = val.split(':', 1)\n",
    "                new_key = parts[0].strip()\n",
    "                expansion = parts[1].strip()\n",
    "            else:\n",
    "                new_key = key\n",
    "                expansion = val.strip()\n",
    "\n",
    "            # Add to new_model_2\n",
    "            if new_key not in new_model_2:\n",
    "                new_model_2[new_key] = []\n",
    "            if expansion not in new_model_2[new_key]:\n",
    "                new_model_2[new_key].append(expansion)\n",
    "\n",
    "    return new_model_2\n",
    "\n",
    "# === Process each entry ===\n",
    "for entry in data:\n",
    "    if isinstance(entry, dict) and \"model_2\" in entry:\n",
    "        entry[\"model_2\"] = restructure_model_2(entry[\"model_2\"])\n",
    "\n",
    "# === Save output ===\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Transformed model_2 structure saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0d3b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== üîç GPT vs Llama-Ft Comparison Across Two Evaluation Calls ===\n",
      "\n",
      "Note: Model labels are aligned. GPT = Model 1 in both, Llama-Ft = Model 2\n",
      "\n",
      "Comparison format: [Data3 ‚Üí Data4]\n",
      "       Comparison  Query Count\n",
      "Model 1 ‚Üí Model 1          284\n",
      "Model 1 ‚Üí Model 2         3175\n",
      "    Model 1 ‚Üí Tie          347\n",
      "Model 2 ‚Üí Model 1           23\n",
      "Model 2 ‚Üí Model 2         1192\n",
      "    Model 2 ‚Üí Tie          103\n",
      "    Tie ‚Üí Model 1           22\n",
      "    Tie ‚Üí Model 2          402\n",
      "        Tie ‚Üí Tie          736\n",
      "\n",
      "=== üìä Confusion Matrix ===\n",
      "Data6    Model 1  Model 2  Tie\n",
      "Data5                         \n",
      "Model 1      284     3175  347\n",
      "Model 2       23     1192  103\n",
      "Tie           22      402  736\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "with open(\"/Users/rishabh.singh/Desktop/ai-search-retrieval-pipeline-poc-2/mismatched_evaluation_results_gpt_llama_nano.json\", 'r') as f :\n",
    "    data5 = json.load(f)\n",
    "\n",
    "with open(\"/Users/rishabh.singh/Desktop/ai-search-retrieval-pipeline-poc-2/mismatched_evaluation_results_gpt_llama_nano_1st.json\", 'r') as f :\n",
    "    data6 = json.load(f)\n",
    "\n",
    "# === Invert better_model label for data3 ===\n",
    "def invert_label(label):\n",
    "    label = label.strip()\n",
    "    if label.lower() == \"model 1\":\n",
    "        return \"Model 2\"\n",
    "    elif label.lower() == \"model 2\":\n",
    "        return \"Model 1\"\n",
    "    else:\n",
    "        return \"Tie\"\n",
    "\n",
    "# === Build mapping: query -> better_model ===\n",
    "def get_better_model_map(data, invert=False):\n",
    "    mapping = {}\n",
    "    for item in data:\n",
    "        query = item[\"query\"]\n",
    "        better_model = item.get(\"better_model\", \"Tie\").strip()\n",
    "        if invert:\n",
    "            better_model = invert_label(better_model)\n",
    "        mapping[query] = better_model\n",
    "    return mapping\n",
    "\n",
    "# In data5: Model 1 = Qwen-Base, Model 2 = GPT ‚Üí so we invert data3\n",
    "# In data6: Model 1 = GPT, Model 2 = Qwen-Base ‚Üí already aligned\n",
    "bm3_map = get_better_model_map(data6, invert=True)   # invert to make GPT = Model 1\n",
    "bm4_map = get_better_model_map(data5, invert=False)  # already aligned\n",
    "\n",
    "# === Compare results for common queries ===\n",
    "counter = Counter()\n",
    "records = []\n",
    "\n",
    "common_queries = set(bm3_map.keys()) & set(bm4_map.keys())\n",
    "\n",
    "for query in common_queries:\n",
    "    bm1 = bm3_map[query]  # From data3 (after inversion)\n",
    "    bm2 = bm4_map[query]  # From data4 (no inversion needed)\n",
    "    \n",
    "    counter[(bm1, bm2)] += 1\n",
    "    records.append({\n",
    "        \"query\": query,\n",
    "        \"Better Model (Data5)\": bm1,\n",
    "        \"Better Model (Data6)\": bm2\n",
    "    })\n",
    "\n",
    "# === Display summary cleanly ===\n",
    "print(\"\\n=== üîç GPT vs Llama-Ft Comparison Across Two Evaluation Calls ===\\n\")\n",
    "print(\"Note: Model labels are aligned. GPT = Model 1 in both, Llama-Ft = Model 2\\n\")\n",
    "print(\"Comparison format: [Data3 ‚Üí Data4]\")\n",
    "\n",
    "summary_table = []\n",
    "for (m1, m2), count in sorted(counter.items()):\n",
    "    label = f\"{m1} ‚Üí {m2}\"\n",
    "    summary_table.append((label, count))\n",
    "\n",
    "# Display as DataFrame\n",
    "df_summary = pd.DataFrame(summary_table, columns=[\"Comparison\", \"Query Count\"])\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "df_compare = pd.DataFrame(records)\n",
    "conf_matrix = pd.crosstab(\n",
    "    df_compare[\"Better Model (Data5)\"],\n",
    "    df_compare[\"Better Model (Data6)\"],\n",
    "    rownames=[\"Data5\"], \n",
    "    colnames=[\"Data6\"],\n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== üìä Confusion Matrix ===\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbafe21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== üîç GPT vs Llama-Ft Comparison Across Two Evaluation Calls ===\n",
      "\n",
      "Note: Model labels are aligned. GPT = Model 1 in both, Llama-Ft = Model 2\n",
      "\n",
      "Comparison format: [Data5 ‚Üí Data6]\n",
      "       Comparison  Query Count\n",
      "Model 1 ‚Üí Model 1          695\n",
      "Model 1 ‚Üí Model 2          199\n",
      "    Model 1 ‚Üí Tie          141\n",
      "Model 2 ‚Üí Model 1         1420\n",
      "Model 2 ‚Üí Model 2         1801\n",
      "    Model 2 ‚Üí Tie          454\n",
      "    Tie ‚Üí Model 1          229\n",
      "    Tie ‚Üí Model 2          166\n",
      "        Tie ‚Üí Tie          516\n",
      "\n",
      "=== üìä Confusion Matrix ===\n",
      "Data6    Model 1  Model 2  Tie\n",
      "Data5                         \n",
      "Model 1      695      199  141\n",
      "Model 2     1420     1801  454\n",
      "Tie          229      166  516\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "with open(\"/Users/rishabh.singh/Desktop/ai-search-retrieval-pipeline-poc-2/mismatched_evaluation_results_gpt_llama_nano_1st_call.json\", 'r') as f :\n",
    "    data5 = json.load(f)\n",
    "\n",
    "with open(\"/Users/rishabh.singh/Desktop/ai-search-retrieval-pipeline-poc-2/mismatched_evaluation_results_gpt_llama_nano_2nd_call.json\", 'r') as f :\n",
    "    data6 = json.load(f)\n",
    "\n",
    "# === Invert better_model label for data3 ===\n",
    "def invert_label(label):\n",
    "    label = label.strip()\n",
    "    if label.lower() == \"model 1\":\n",
    "        return \"Model 2\"\n",
    "    elif label.lower() == \"model 2\":\n",
    "        return \"Model 1\"\n",
    "    else:\n",
    "        return \"Tie\"\n",
    "\n",
    "# === Build mapping: query -> better_model ===\n",
    "def get_better_model_map(data, invert=False):\n",
    "    mapping = {}\n",
    "    for item in data:\n",
    "        query = item[\"query\"]\n",
    "        better_model = item.get(\"better_model\", \"Tie\").strip()\n",
    "        if invert:\n",
    "            better_model = invert_label(better_model)\n",
    "        mapping[query] = better_model\n",
    "    return mapping\n",
    "\n",
    "# In data5: Model 1 = GPT, Model 2 = Llama-Ft ‚Üí so we invert data3\n",
    "# In data6: Model 1 = Llama-Ft, Model 2 = GPT ‚Üí already aligned\n",
    "bm3_map = get_better_model_map(data5, invert=False)   # invert to make GPT = Model 1\n",
    "bm4_map = get_better_model_map(data6, invert=True)  # already aligned\n",
    "\n",
    "# === Compare results for common queries ===\n",
    "counter = Counter()\n",
    "records = []\n",
    "\n",
    "common_queries = set(bm3_map.keys()) & set(bm4_map.keys())\n",
    "\n",
    "for query in common_queries:\n",
    "    bm1 = bm3_map[query]  # From data3 (after inversion)\n",
    "    bm2 = bm4_map[query]  # From data4 (no inversion needed)\n",
    "    \n",
    "    counter[(bm1, bm2)] += 1\n",
    "    records.append({\n",
    "        \"query\": query,\n",
    "        \"Better Model (Data5)\": bm1,\n",
    "        \"Better Model (Data6)\": bm2\n",
    "    })\n",
    "\n",
    "# === Display summary cleanly ===\n",
    "print(\"\\n=== üîç GPT vs Llama-Ft Comparison Across Two Evaluation Calls ===\\n\")\n",
    "print(\"Note: Model labels are aligned. GPT = Model 1 in both, Llama-Ft = Model 2\\n\")\n",
    "print(\"Comparison format: [Data5 ‚Üí Data6]\")\n",
    "\n",
    "summary_table = []\n",
    "for (m1, m2), count in sorted(counter.items()):\n",
    "    label = f\"{m1} ‚Üí {m2}\"\n",
    "    summary_table.append((label, count))\n",
    "\n",
    "# Display as DataFrame\n",
    "df_summary = pd.DataFrame(summary_table, columns=[\"Comparison\", \"Query Count\"])\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "df_compare = pd.DataFrame(records)\n",
    "conf_matrix = pd.crosstab(\n",
    "    df_compare[\"Better Model (Data5)\"],\n",
    "    df_compare[\"Better Model (Data6)\"],\n",
    "    rownames=[\"Data5\"], \n",
    "    colnames=[\"Data6\"],\n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== üìä Confusion Matrix ===\")\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
